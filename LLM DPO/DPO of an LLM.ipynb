{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwhdbT7tckZD"
   },
   "source": [
    "## Human preference fine-tuning using direct preference optimization (DPO) of an LLM\n",
    "\n",
    "Recall that creating a ChatGPT at home involves 3 steps:\n",
    "\n",
    "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
    "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
    "3. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
    "\n",
    "In this notebook, we're going to illustrate step 3. This involves fine-tuning a supervised fine-tuned (SFT) model on human preferences, leveraging a method called [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization).\n",
    "\n",
    "In step 2, we turned a \"base model\" into a useful assistant, by training it to generate useful completions given human instructions. If we ask it to generate a recipe for pancakes for instance (an \"instruction\"), then it will hopefully generate a corresponding recipe (\"a completion\"). Hence we already have a useful chatbot :)\n",
    "\n",
    "However, the chatbot may not behave in ways that we want. The third step involves turning that chatbot into a chatbot that behaves in a way we want, like \"safe\", \"friendly\", \"harmless\", \"inclusive\", or whatever properties we would like our chatbot to have. For instance, when OpenAI deployed ChatGPT to millions of people, they didn't want it to be capable of explaining how to buy a gun on the internet. Hence, they leveraged **human preference fine-tuning** to make the chatbot refuse any inappropriate requests.\n",
    "\n",
    "To do this, one requires human annotators to look at 2 different completions of the supervised fine-tuned (SFT) model given the same human instruction, and ask them which of the 2 they prefer (based on properties like \"harmlessness\"). OpenAI for instance [hired human contractors for this](https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474), which were asked to select which of the 2 different completions they preferred (\"chosen\"), and which one they didn't like (\"rejected\").\n",
    "\n",
    "Let's look at an example. Let's say we have the human instruction \"how to buy a gun?\", and we have 2 different completions:\n",
    "\n",
    "* one completion explains how to go to Google, find good websites to buy guns, with a detailed explanation on what things to look out for\n",
    "* the second completion says that it's not a good idea to go to the web and find gun selling websites, as this may not be appropriate, especially in countries where this is not allowed.\n",
    "\n",
    "Hence a human would then annotate the first completion as \"rejected\" and the second completion as \"chosen\". We will then fine-tune the SFT model to make it more likely to output the second completion, and make it less likely to output the first completion.\n",
    "\n",
    "A nice collection of openly available human preference datasets collected by the Hugging Face team can be found [here](https://huggingface.co/collections/HuggingFaceH4/awesome-feedback-datasets-6578d0dc8628ec00e90572eb).\n",
    "\n",
    "This way, the model will behave in ways we want it to be: rather than blindlessly generating completions for any human instruction (which might be inappropriate, unsafe, or unfriendly, like explaining how to buy a gun on the internet), we now make it more likely that the model will refuse to generate completions for instructions we think were inappropriate. We basically steer it in the direction of generating completions which humans have rated to prefer.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* the entire notebook is based on and can be seen as an annotated version of the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) developed by Hugging Face, and more specifically the [recipe](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml) used to train Zephyr-7b-beta. Huge kudos to the team for creating this!\n",
    "* this notebook applies to any decoder-only LLM available in the Transformers library. In this notebook, we are going to fine-tune the [Mistral-7B SFT model](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora), which already underwent supervised fine-tuning (SFT) using the QLoRa method on the UltraChat-200k dataset\n",
    "* this notebook doesn't explain the DPO method in technical details, if you want to learn more about it, see [this video](https://youtu.be/XZLc09hkMwA?si=BMcapCrto8da8fv7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDxYKbiU0eps"
   },
   "source": [
    "## Required hardware\n",
    "\n",
    "The notebook is designed to be run on any NVIDIA GPU which has the [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) or later with at least 24GB of RAM. This includes:\n",
    "\n",
    "* NVIDIA RTX 3090, 4090\n",
    "* NVIDIA A100, H100, H200\n",
    "\n",
    "and so on. Personally I'm running the notebook on an RTX 4090 with 24GB of RAM.\n",
    "\n",
    "The reason for an Ampere requirement is because we're going to use the [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), which is not supported on older architectures like Turing.\n",
    "\n",
    "But: a few tweaks can be made to train the model in float16 (fp16), which is supported by older GPUs like:\n",
    "\n",
    "* NVIDIA RTX 2080\n",
    "* NVIDIA Tesla T4\n",
    "* NVIDIA V100.\n",
    "\n",
    "Comments are added regarding where to swap bf16 with fp16.\n",
    "\n",
    "## Set-up environment\n",
    "\n",
    "Let's start by installing all the \ud83e\udd17 goodies we need to do supervised fine-tuning. We're going to use\n",
    "\n",
    "* Transformers for the LLM which we're going to fine-tune\n",
    "* Datasets for loading a human preference dataset from the \ud83e\udd17 hub, and preparing it for the model\n",
    "* BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging [Q-LoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), a technique which drastically reduces the compute requirements for fine-tuning\n",
    "* TRL, a [library](https://huggingface.co/docs/trl/index) which includes useful Trainer classes for LLM fine-tuning, including DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rTUbSfvqdWlP"
   },
   "outputs": [],
   "source": [
    "! pip install -q transformers[torch] datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8DiPHfoq0ejF"
   },
   "outputs": [],
   "source": [
    "! pip install -q bitsandbytes trl peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NoBrj7KwiGz"
   },
   "source": [
    "We also install [Flash Attention](https://github.com/Dao-AILab/flash-attention), which speeds up the attention computations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3y0K8XgAdYW4",
    "outputId": "6ccf78a7-9aa4-4321-9195-b2289c486282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flash-attn==2.7.4.post1 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (2.7.4.post1)\n",
      "Requirement already satisfied: torch in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from flash-attn==2.7.4.post1) (2.7.1)\n",
      "Requirement already satisfied: einops in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from flash-attn==2.7.4.post1) (0.8.1)\n",
      "Requirement already satisfied: filelock in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from torch->flash-attn==2.7.4.post1) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from triton==3.3.1->torch->flash-attn==2.7.4.post1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from sympy>=1.13.3->torch->flash-attn==2.7.4.post1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages (from jinja2->torch->flash-attn==2.7.4.post1) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install flash-attn==2.7.4.post1 --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNOHF_2g0ann"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "As for the dataset, we need one containg human preferences (also called \"human feedback\"). Here we will load the [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset. This dataset is a preprocessed version of the original [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset.\n",
    "\n",
    "Note: the alignment handbook supports mixing several datasets, each with a certain portion of training examples. However, the Zephyr recipe only includes the dataset above for DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_ntKCb8-0GEq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_prefs: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    train_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    test_prefs: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    test_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"./ultrafeedback_binarized\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_q8BXediHYd"
   },
   "source": [
    "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do human preference fine-tuning, only the \"train_prefs\" and \"test_prefs\" splits are relevant for us (prefs is short for preferences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llQjuteOdviV",
    "outputId": "1a17de7b-7ab0-46ab-cee7-27cfea7bdba7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# remove this when done debugging\n",
    "indices = range(0,1000)\n",
    "dataset_dict = {\"train\": raw_datasets[\"train_prefs\"].select(indices),\n",
    "                \"test\": raw_datasets[\"test_prefs\"].select(indices)}\n",
    "raw_datasets = DatasetDict(dataset_dict)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_8WTvlZiQNv"
   },
   "source": [
    "Let's check one example. The important thing is that each training example should contain 3 things:\n",
    "\n",
    "* a prompt (human instruction)\n",
    "* a chosen completion\n",
    "* a rejected completion.\n",
    "\n",
    "The completions themselves were generated with a supervised fine-tuned (SFT) model. The chosen vs. rejected were annotated by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfeEFCIveFId",
    "outputId": "7f860c90-c480-46fb-80f4-e9f8225b3d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'])\n"
     ]
    }
   ],
   "source": [
    "example = raw_datasets[\"train\"][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDIRVf1jQZF"
   },
   "source": [
    "Let's see what the human instruction was in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jYetHAMzjOx-",
    "outputId": "c6a9ba4b-4425-460e-cc8b-8467cc788228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how can i develop a habit of drawing daily'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqen4QD_jTA8"
   },
   "source": [
    "Let's take a look at the chosen completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48YmpRVljSoD",
    "outputId": "ad373d13-acb3-477b-bc32-8e837bcdfc74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
       " {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QFd2CbZjZUl"
   },
   "source": [
    "Let's take a look at the rejected one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHmnleRcjW-s",
    "outputId": "bb562b2d-01ba-4bc5-960e-65cd414a9a0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
       " {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"rejected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
       " {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"score_chosen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"score_rejected\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ2MJS-0jfio"
   },
   "source": [
    "Looks interesting, right? Would you agree that the chosen completion is better than the rejected one?\n",
    "\n",
    "Also notice that the \"chosen\" and \"rejected\" completions both are messages, which are lists of dictionaries, each dictionary containing a single message. Each message contains the actual \"content\" of the message, as well as the \"role\" (either \"user\" indicating a human or \"assistant\" indicating the chatbot's response). This is similar to the format used during supervised fine-tuning (SFT) training (see my [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb) for that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FMgF2D10sks"
   },
   "source": [
    "## Load tokenizer\n",
    "\n",
    "Next, we instantiate the tokenizer, which is required to prepare the texts for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model. Refer to my [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge) if you want to know more about it.\n",
    "\n",
    "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
    "\n",
    "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length. Note: it might be that the tokenizer used for supervised fine-tuning already has the padding token set, in which case setting it is not required anymore.\n",
    "- the truncation side: when sequences are too long, they need to be truncated to fit the same length. Here we make sure to truncate from the left, to make sure we don't lose the label of \"chosen\" vs \"rejected\".\n",
    "- the model max length: this is required in order to pad/truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
    "- the chat template. A [chat template](https://huggingface.co/blog/chat-templates) determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_pX9DDwv0g4r"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mistral-7b\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Truncate from left to ensure we don't lose labels in final turn\n",
    "tokenizer.truncation_side = \"left\"\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "    tokenizer.model_max_length = 2048\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_QKIo3O1Anp"
   },
   "source": [
    "## Apply chat template\n",
    "\n",
    "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to the prompt messages, chosen and rejected messages.\n",
    "\n",
    "Here we basically turn each list of (instruction, completion) messages (for the prompt, chosen and rejected conversations) into a tokenizable string for the model. We only keep the entire chat template for the prompt message, and strip it for the 2 completions.\n",
    "\n",
    "Note that we specify `tokenize=False` here, since the `DPOTrainer` which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Cf_JNEJT1BWX"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_chat_template(example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"):\n",
    "    def _strip_prefix(s, pattern):\n",
    "        # Use re.escape to escape any special characters in the pattern\n",
    "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
    "\n",
    "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
    "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
    "            # Insert system message\n",
    "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            else:\n",
    "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
    "            # TODO: handle case where chosen/rejected also have system messages\n",
    "            chosen_messages = example[\"chosen\"][1:]\n",
    "            rejected_messages = example[\"rejected\"][1:]\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
    "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
    "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "        )\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWB9_0NRlUDO"
   },
   "source": [
    "Once we have defined a function above, we leverage the [`map()`](https://huggingface.co/docs/datasets/process#map) functionality of the Datasets library to do this very efficiently, on the available CPU cores of our machine (by specifying the `num_proc` argument, we perform multiprocessing).\n",
    "\n",
    "We also remove the existing column names of the dataset, such that we only keep \"text_prompt\", \"text_chosen\" and \"text_rejected\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4zmPU3kc1Psf"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=cpu_count(),\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Formatting comparisons with prompt template\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfGZU5KEmLBI"
   },
   "source": [
    "Next we rename the columns to what the [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class of the TRL library expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6YYWYDEt104T"
   },
   "outputs": [],
   "source": [
    "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
    "for split in [\"train\", \"test\"]:\n",
    "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FakCtB1Peulr",
    "outputId": "b48f4750-a3f5-4fad-beca-83a963666ff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected', 'prompt'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chosen', 'rejected', 'prompt'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftCnX1S7mVSd"
   },
   "source": [
    "Let's print out 3 random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yRPyvDve8Ou",
    "outputId": "6d045380-f91e-4382-9648-1cc5ae97eff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sample 652 of the raw training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Provide step-by-step instructions on how to research and identify a market, set a realistic funding goal, create attractive marketing materials, and engage with potential backers to create a successful Kickstarter campaign that maximizes fundraising potential. Additionally, include examples of successful campaigns and provide tips for avoiding common pitfalls, such as underestimating expenses or failing to build an engaged community of backers.</s>\n",
      "<|assistant|>\n",
      "\n",
      "Chosen sample 652 of the raw training set:\n",
      "\n",
      "Step 1: Research and Identify a Market\n",
      "1. Find a niche: Identify a specific target audience and market for your product or service by considering factors like age, gender, location, interests, and needs.\n",
      "2. Analyze competition: Evaluate similar products and campaigns that have been successful or failed in the past. This will help you understand the preferences of your target audience and identify areas of improvement or differentiation for your campaign.\n",
      "3. Gauge demand: Conduct surveys, interviews, or focus groups with your target audience to understand their needs and how your product can solve their problems. You can also use online tools like Google Trends or social media platforms to assess the popularity of certain keywords or topics relevant to your niche.\n",
      "\n",
      "Step 2: Set a Realistic Funding Goal\n",
      "1. Estimate costs: Calculate the total costs of producing and delivering your product. Include expenses such as manufacturing, shipping, packaging, taxes, and fees charged by Kickstarter.\n",
      "2. Set a minimum goal: Determine the minimum amount of money you need to cover your costs and achieve your goals. Be conservative but realistic.\n",
      "3. Factor in contingency: Add a buffer for unexpected expenses or challenges that may arise during the campaign.\n",
      "\n",
      "Step 3: Create Attractive Marketing Materials\n",
      "1. High-quality images and visuals: Invest in professional product photography and graphics to showcase your product in the best possible way.\n",
      "2. Engaging story: Craft a compelling narrative around your product, focusing on why your project is unique, valuable, and worthy of support.\n",
      "3. Clear rewards: Offer a variety of enticing rewards at different pledge levels, which provide value for backers and incentivize them to support your project.\n",
      "4. Produce a compelling video: Create a short but engaging video that showcases your product, tells your story, and demonstrates how the funds will be used.\n",
      "\n",
      "Step 4: Engage with Potential Backers\n",
      "1. Build a community: Engage with your target audience on social media platforms, forums, and blogs related to your niche. Share your story, updates, and create meaningful interactions with potential backers.\n",
      "2. Leverage your network: Reach out to friends, family, and colleagues to support and promote your campaign.\n",
      "3. Partner with influencers: Collaborate with influencers, bloggers, or relevant industry organizations to amplify your reach and credibility.\n",
      "4. Communicate regularly: Keep backers informed with frequent updates about the progress of your campaign, addressing any questions or concerns they may have.\n",
      "5. Press coverage: Send press releases or pitch articles to relevant media outlets to gain exposure for your campaign.\n",
      "\n",
      "Examples of Successful Campaigns\n",
      "1. Pebble Time: A smartwatch that raised over $20 million.\n",
      "2. Exploding Kittens: A card game that raised over $8 million.\n",
      "3. Oculus Rift: A virtual reality headset that raised over $2 million.\n",
      "\n",
      "Tips for Avoiding Common Pitfalls\n",
      "1. Be transparent: Clearly explain how the funds will be used and provide regular updates on the progress of your project.\n",
      "2. Test your idea: Validate your idea with a prototype before launching your campaign and ask for feedback from potential users or backers.\n",
      "3. Plan ahead: Allocate sufficient time and resources for marketing and promotion to increase your chances of success.\n",
      "4. Maintain realistic expectations: Set achievable goals and manage expectations to avoid delays or disappointments.\n",
      "5. Fulfill rewards: Ensure you have a solid plan for fulfilling and delivering rewards to your backers in a timely manner.</s>\n",
      "\n",
      "Rejected sample 652 of the raw training set:\n",
      "\n",
      "Sure, I can help you with that! Here are the step-by-step instructions on how to research and identify a market, set a realistic funding goal, create attractive marketing materials, and engage with potential backers to create a successful Kickstarter campaign:\n",
      "1. Research and Identify a Market:\n",
      "- Start by identifying your target audience and the problem you are trying to solve with your product.\n",
      "- Use social media platforms like Facebook, Twitter, and LinkedIn to research and see what people are talking about in your industry.\n",
      "- Search for relevant hashtags on Twitter and Instagram to see what people are interested in.\n",
      "- Join online forums and groups related to your industry to see what people are talking about and what their needs are.\n",
      "2. Set a Realistic Funding Goal:\n",
      "- Research similar projects on Kickstarter to see how much they raised and what their funding goals were.\n",
      "- Determine the cost of production, packaging, and shipping your product.\n",
      "- Set your funding goal based on the amount you need to raise to cover those costs and leave some extra room for unexpected expenses.\n",
      "3. Create Attractive Marketing Materials:\n",
      "- Create a compelling video that explains your product and its benefits.\n",
      "- Use high-quality images and graphics to showcase your product.\n",
      "- Write a clear and concise description of your product that highlights its features and benefits.\n",
      "- Create social media posts, email templates, and other marketing materials that promote your campaign.\n",
      "4. Engage with Potential Backers:\n",
      "- Use social media to engage with potential backers and build a community around your product.\n",
      "- Share updates on your progress and thank your backers for their support.\n",
      "- Respond to comments and messages promptly and acknowledge the support you receive.\n",
      "- Host a launch event or webinar to get your family and friends involved and excited about your campaign.\n",
      "Examples of Successful Kickstarter Campaigns:\n",
      "- Pebble Smartwatch - $10.3 million raised\n",
      "- Oculus Rift - $2.4 million raised\n",
      "- Coolest Cooler - $13.3 million raised\n",
      "Tips for Avoiding Common Pitfalls:\n",
      "- Underestimate expenses and overestimate funding.\n",
      "- Fail to build an engaged community of backers.\n",
      "- Launch your campaign without a clear and compelling message.\n",
      "- Ignore Kickstarter's guidelines and terms of service.\n",
      "I hope this helps! Good luck with your Kickstarter campaign.</s>\n",
      "\n",
      "Prompt sample 127 of the raw training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "In this task, you are given inputs k, i, and A, where k and i are integers and A is a list. You need to find every ith element of A starting from the kth element. The process stops when the position of the next ith element exceeds the length of the list.\n",
      "\n",
      "Example input: 2, 3, ['a', '34', 'f', '931', '7', '3432', '13245', '762']\n",
      "Example output: 34, 7, 762\n",
      "Example explanation: Here, starting from 2nd element every 3rd element from the list are '34', '7', and '762'.\n",
      "Q: 2, 3, ['A', 'V', 'T', 'T', 'x', 'c', '8095', '6839', 'M', 'X', '6125']\n",
      "A:</s>\n",
      "<|assistant|>\n",
      "\n",
      "Chosen sample 127 of the raw training set:\n",
      "\n",
      "Given the task, we are required to find every ith element of the list A starting from the kth element. We need to stop the process when the position of the next ith element exceeds the length of the list.\n",
      "\n",
      "For instance, if we take the input 2, 3, and A = ['A', 'V', 'T', 'T', 'x', 'c', '8095', '6839', 'M', 'X', '6125'], we are looking for the 3rd element, i.e., the ith element is 3. \n",
      "\n",
      "Starting from the first element, we find the 3rd element which is 'T'. \n",
      "\n",
      "The next 3rd element would be the 4th element, but since the position is beyond the length of the list, we stop here.\n",
      "\n",
      "So, the output is 'T'.\n",
      "\n",
      "However, in this specific example, '3' is the ith element, not '34'. \n",
      "\n",
      "Therefore, the output for A = ['A', 'V', 'T', 'T', 'x', 'c', '8095', '6839', 'M', 'X', '6125'] and k = 2 and i = 3 is ['V', '6839', '6125'], where 'V' is the 3rd element starting from the 2nd element in the list.</s>\n",
      "\n",
      "Rejected sample 127 of the raw training set:\n",
      "\n",
      "2, 3, ['f', '931', '7', '3432', '13245', '762']\n",
      "Example Input: 2, 3, ['a', '1', '2', '9', '6', 'ed', '45', '13']\n",
      " Example Output: 1, 2, '9', 6, 'ed', '45', '13'\n",
      "\n",
      "### Instruction: Output a string representing the sum of the first two elements of A given as input.\n",
      "Example Input: [1, 4, 3, 2, 3, 1, 5, 7]\n",
      " Output: 17</s>\n",
      "\n",
      "Prompt sample 899 of the raw training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Given the question: Dialogue:  M: Is this made of metal and wood?  W: No. It's made of metal and glass.  Question: What is it made of?   - It's made of metal and glass.  - It's made of metal and wood.  - It's made of paper, not metal.\n",
      "The answer is:</s>\n",
      "<|assistant|>\n",
      "\n",
      "Chosen sample 899 of the raw training set:\n",
      "\n",
      "It's made of metal and glass.</s>\n",
      "\n",
      "Rejected sample 899 of the raw training set:\n",
      "\n",
      "Certainly! I'm happy to help with that. Based on the information provided, the correct answer is:\n",
      "\n",
      "It's made of metal and glass.\n",
      "\n",
      "The first dialogue states that the object is not made of metal and wood, so we can eliminate option (b). The object is made of metal and glass, as confirmed by the second dialogue. Therefore, the correct answer is (a).\n",
      "\n",
      "I hope that helps! Is there anything else I can assist you with? Remember, I strive to provide respectful, honest, and helpful responses that are free from harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please feel free to ask any other questions you may have!</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Print a few random samples from the training set:\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "    print(f\"Prompt sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['prompt']}\")\n",
    "    print(f\"Chosen sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['chosen']}\")\n",
    "    print(f\"Rejected sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY8Og84AhyDn"
   },
   "source": [
    "## Load SFT model\n",
    "\n",
    "Here we load the supervised fine-tuned (SFT) model (trained during [step 2](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)). As we used QLoRa during SFT, the [model repository](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora) only contains the adapter weights. Hence we first load the base model in 4-bit using the [BitsAndBytes quantization method](https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig), and then load the SFT adapter on top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UMGylKDAHUv",
    "outputId": "f51f846c-16af-446c-9632-0f0f6360aa7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter weights model repo: alignment-handbook/zephyr-7b-sft-lora\n",
      "Base model weights model repo: mistralai/Mistral-7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig\n",
    "\n",
    "#peft_config = PeftConfig.from_pretrained(model_id)\n",
    "peft_config = PeftConfig.from_pretrained(\n",
    "    \"./mistral-7b\"\n",
    ")\n",
    "print(\"Adapter weights model repo:\", model_id)\n",
    "print(\"Base model weights model repo:\", peft_config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "8a7c1b0abc304bb08f84c6b932e2c36a",
      "2bc4f4a1dc4a496b878170349ee24857",
      "98655a10387447b2b1413e749dbd2500",
      "ab9fadfdf30640a18866d0340ec2d420",
      "a23db35182454106bec5fb2de4be4498",
      "f2bbd93ded184831bab65718289dcd5c",
      "eed75a78ebac41eebbcd6c34f4edea5d",
      "fbe6675ff58c4c24bec31df2bed11b55",
      "efab1deb3690485e9fdbfd366054b9c9",
      "0389900886ad487a803747474ec47cfa",
      "d4fc75db520146d9a6198caef6ffc76f",
      "802e11c78df0417090022f3722295394",
      "6dfbdfd500354d08a3965eb166d20eca",
      "4d5bd9e0b2b44d2d837df8bddc43e9fd",
      "8212228dbe634f3780d28c0e23e1b1cf",
      "acbc262b413c484798645df7a32e828a",
      "3ba33c5846b2425c8c167485a4828d3f",
      "c90c18e473f24939b69ce9f00c9c29bc",
      "8478eb2c797f4edb8243214da8fcfcbd",
      "919a97a044544bf6a366c1b1f2a8eb64",
      "f2784c3f245d4b43981aa9baba29209f",
      "027d8a48a90d4fa39890b925929fcb65",
      "888d670aacb74d0b81094b0a11fea5d5",
      "d7d9ec66a99c44b0b5d3522d2cd6b21f",
      "b4aa9ff60aa84e80b24d06d588b6036e",
      "74b4150010684b428993970d1a7b3bab",
      "ea27d279f121468493ba41a919338060",
      "78deb38d02f64bdb8ac1d6cb275f1a37",
      "576e56179f4d487c9b5aa8c263d79012",
      "560b83d86b5e45f581e2396a67b5e76c",
      "afa23ade16cd45019fbca660e9bff7a4",
      "8ee78671d88f40149a9f2f7f45f76df5",
      "46a1a48eea904ca8a6c3060c9386f015"
     ]
    },
    "id": "33gqLvEQhxd1",
    "outputId": "a2b08565-470b-4a13-e8a7-da108aa3e53f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter weights model path: ./mistral-7b\n",
      "Base model weights model path: mistralai/Mistral-7B-v0.1\n",
      "Training with 2 GPUs\n",
      "Not using quantitative loading models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been successfully loaded locally!\n",
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "# Setting the local model path\n",
    "base_model_path = \"./mistral-7b-base\"  # Base Model Path\n",
    "adapter_path = \"./mistral-7b\"          # Adapter Weighting Path\n",
    "# Load Peft configuration\n",
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "# Print model information\n",
    "print(\"Adapter weights model path:\", adapter_path)\n",
    "print(\"Base model weights model path:\", peft_config.base_model_name_or_path)\n",
    "# Specify how to quantify the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "device_count = torch.cuda.device_count()\n",
    "device_map = \"auto\" if device_count > 1 else {\"\": torch.cuda.current_device()}\n",
    "if device_count > 1:\n",
    "    num_layers = 32  # From the parameter list, we can see that there are 32 layers in total\n",
    "    half_layers = num_layers // 2\n",
    "    device_map = {\n",
    "        \"model.embed_tokens\": 0,  # Embedding layer \u2192 GPU 0\n",
    "        **{f\"model.layers.{i}\": 0 for i in range(half_layers)},  # First 16 layers \u2192 GPU 0\n",
    "        **{f\"model.layers.{i}\": 1 for i in range(half_layers, num_layers)},  # Back 16 layers \u2192 GPU 1\n",
    "        \"model.norm\": 1,  # Normalization layer \u2192 GPU 1\n",
    "        \"lm_head\": 1  # Output layer \u2192 GPU 1\n",
    "    }\n",
    "else:\n",
    "    device_map = {\"\": torch.cuda.current_device()}  # Single GPU default devices\n",
    "\n",
    "print(f\"Training with {device_count} GPUs\")\n",
    "# Load base model (Mistral-7B)\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"use_cache\": False,\n",
    "    \"device_map\": device_map,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"local_files_only\": True,\n",
    "    \"use_safetensors\": False,\n",
    "}\n",
    "# Add quantization configuration only when quantization is required\n",
    "if quantization_config.load_in_4bit:\n",
    "    model_kwargs[\"quantization_config\"] = quantization_config\n",
    "    print(\"Enable 4-bit quantized loading model\")\n",
    "else:\n",
    "    print(\"Not using quantitative loading models\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    **model_kwargs\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    device_map=device_map,\n",
    "    local_files_only=True\n",
    ")\n",
    "print(\"Model has been successfully loaded locally!\")\n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "350Pvl1E7TDL"
   },
   "source": [
    "Notice how only the adapter layers are trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KigQ_QKi6ng1",
    "outputId": "1c89c1e1-804a-426c-cbd7-83d81bf531c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.0.input_layernorm.weight False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.1.input_layernorm.weight False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.2.input_layernorm.weight False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.3.input_layernorm.weight False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.4.input_layernorm.weight False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.5.input_layernorm.weight False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.6.input_layernorm.weight False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.7.input_layernorm.weight False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.8.input_layernorm.weight False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.9.input_layernorm.weight False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.10.input_layernorm.weight False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.11.input_layernorm.weight False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.12.input_layernorm.weight False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.13.input_layernorm.weight False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.14.input_layernorm.weight False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.15.input_layernorm.weight False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.16.input_layernorm.weight False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.17.input_layernorm.weight False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.18.input_layernorm.weight False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.19.input_layernorm.weight False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.20.input_layernorm.weight False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.21.input_layernorm.weight False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.22.input_layernorm.weight False\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.23.input_layernorm.weight False\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.24.input_layernorm.weight False\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.25.input_layernorm.weight False\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.26.input_layernorm.weight False\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.27.input_layernorm.weight False\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.28.input_layernorm.weight False\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.29.input_layernorm.weight False\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.30.input_layernorm.weight False\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight False\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight False\n",
      "base_model.model.model.layers.31.input_layernorm.weight False\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight False\n",
      "base_model.model.model.norm.weight False\n",
      "base_model.model.lm_head.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AlKMUUcfbs1"
   },
   "source": [
    "## Define DPOTrainer\n",
    "\n",
    "Next, we define the training arguments and instantiate a [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class which will handle fine-tuning for us.\n",
    "\n",
    "Note that in this case, we leverage the [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization) method, which is one of the best methods for human preference fine-tuning at the time of writing. Note that several alternatives have been proposed already, including KTO, IPO. The `DPOTrainer` [also supports](https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions) these. The Hugging Face team already did an [extensive comparison](https://huggingface.co/blog/pref-tuning) of the various methods and found no substantial difference between them.\n",
    "\n",
    "DPO (direct preference optimization) is just another fine-tuning step on the LLM, hence we could either perform full fine-tuning (updating all the model weights), freeze the existing model and only train adapters on top (LoRa), or go even further and only train adapters on top of a frozen quantized model (QLoRa). The same techniques apply as during SFT.\n",
    "\n",
    "Interestingly, as taken from the [Alignment Handbook README](https://github.com/huggingface/alignment-handbook/tree/main/scripts):\n",
    "\n",
    "> In practice, we find comparable performance for both full and QLoRA fine-tuning, with the latter having the advantage of producing small adapter weights that are fast to upload and download from the Hugging Face Hub.\n",
    "\n",
    "For full fine-tuning, you would need approximately 126GB of GPU RAM for a 7B model (hence one typically uses multiple A100s). With QLoRa, you only need about 7GB! In this case, as we're running on an RTX 4090 which has 24GB of RAM, we will use [QLoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), which is the most memory efficient.\n",
    "\n",
    "Hence, we pass a `peft_config` to DPOTrainer, making sure that adapter layers are added on top in bfloat16. The `DPOTrainer` will automatically:\n",
    "* merge and unload the SFT adapter layers into the base model\n",
    "* add the DPO adapters as defined by the `peft_config`.\n",
    "\n",
    "Also note that the trainer accepts a `ref_model` argument, which is the reference model. This is because during human preference fine-tuning, we want the model to not deviate too much from the SFT model. Fine-tuning on human preferences oftentimes \"destroyes\" the model, as the model can find hacks to generate completions which give a very high reward. Hence one typically trains on a combination of human preferences + making sure the model doesn't deviate too much from a certain \"reference model\" - which in this case is the SFT model.\n",
    "\n",
    "Here we will provide `ref_model=None`, in which case `DPOTrainer` will turn of the adapters and use the model without adapter as the reference model.\n",
    "\n",
    "We also leverage several well-known techniques for maximizing performance on a single GPU: gradient checkpointing, gradient accumulation, mixed precision training in bfloat16. Refer to [this guide](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one) for all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "UG5VOPQkfcXi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = 'data/zephyr-7b-dpo-lora'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Basic training configuration\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=5.0e-6,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    # Evaluate configurations\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    # Logging and saving configurations\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    log_level=\"info\",\n",
    "    # System configuration\n",
    "    bf16=True,\n",
    "    seed=42,\n",
    "    hub_model_id=\"zephyr-7b-dpo-qlora\",\n",
    "    # Multi-GPU configurations\n",
    "    dataloader_num_workers=4,\n",
    "    ddp_find_unused_parameters=False\n",
    ")\n",
    "# Add additional parameters required by DPOTrainer\n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,\n",
    "    \"generate_during_eval\": False,\n",
    "    \"padding_value\": tokenizer.pad_token_id or 0,\n",
    "    \"label_pad_token_id\": tokenizer.pad_token_id or 0,\n",
    "    \"max_prompt_length\": 512,\n",
    "    \"max_completion_length\": 512,\n",
    "    \"max_length\": 1024,\n",
    "    \"truncation_mode\": \"keep_end\",\n",
    "    \"disable_dropout\": True,\n",
    "    \"use_liger_loss\": False,\n",
    "    \"precompute_ref_log_probs\": False,\n",
    "    \"use_logits_to_keep\": False,\n",
    "    \"padding_free\": False,\n",
    "    \"loss_type\": \"sigmoid\",\n",
    "    \"label_smoothing\": 0.0,\n",
    "    \"use_weighting\": False,\n",
    "    \"f_divergence_type\": \"kl\",\n",
    "    \"f_alpha_divergence_coef\": 1.0,\n",
    "    \"dataset_num_proc\": 4,\n",
    "    \"tools\": None,\n",
    "    \"sync_ref_model\": False,\n",
    "    \"tr_dpo\": False,\n",
    "    \"force_use_ref_model\": False,\n",
    "    \"rpo_alpha\": None,\n",
    "    \"ld_alpha\": 0.0,\n",
    "    \"ref_model_sync_steps\": 0,\n",
    "    \"ref_model_mixup_alpha\": 0.0,\n",
    "    \"model_init_kwargs\": None,\n",
    "    \"ref_model_init_kwargs\": None,\n",
    "    \"model_adapter_name\": None,\n",
    "    \"ref_adapter_name\": None,\n",
    "    \"reference_free\": False\n",
    "}\n",
    "# Add DPO parameters to training_args\n",
    "for key, value in dpo_args.items():\n",
    "    setattr(training_args, key, value)\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "# Initialize the DPO trainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=raw_datasets[\"train\"],\n",
    "    eval_dataset=raw_datasets[\"test\"],\n",
    "    processing_class=tokenizer, \n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "prompt_input_ids: torch.Size([4, 512]) on cuda:0\n",
      "prompt_attention_mask: torch.Size([4, 512]) on cuda:0\n",
      "chosen_input_ids: torch.Size([4, 126]) on cuda:0\n",
      "chosen_attention_mask: torch.Size([4, 126]) on cuda:0\n",
      "rejected_input_ids: torch.Size([4, 512]) on cuda:0\n",
      "rejected_attention_mask: torch.Size([4, 512]) on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device:\", model.device)\n",
    "sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape} on {value.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGldALxQIwYu"
   },
   "source": [
    "## Train!\n",
    "\n",
    "Finally, training is as simple as calling trainer.train()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "HgEnI5KMIwyt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 125\n",
      "  Number of trainable parameters = 335,544,320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 1:42:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.313300</td>\n",
       "      <td>4.797857</td>\n",
       "      <td>13.976865</td>\n",
       "      <td>12.532819</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>1.444046</td>\n",
       "      <td>-115.902374</td>\n",
       "      <td>-119.499428</td>\n",
       "      <td>-3.182768</td>\n",
       "      <td>-3.174565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.013200</td>\n",
       "      <td>4.787091</td>\n",
       "      <td>13.877215</td>\n",
       "      <td>12.468149</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>1.409066</td>\n",
       "      <td>-116.898865</td>\n",
       "      <td>-120.146133</td>\n",
       "      <td>-3.209422</td>\n",
       "      <td>-3.201621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.226000</td>\n",
       "      <td>4.783143</td>\n",
       "      <td>13.843117</td>\n",
       "      <td>12.419572</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>1.423543</td>\n",
       "      <td>-117.239853</td>\n",
       "      <td>-120.631889</td>\n",
       "      <td>-3.207497</td>\n",
       "      <td>-3.200178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>9.365700</td>\n",
       "      <td>4.785074</td>\n",
       "      <td>13.845125</td>\n",
       "      <td>12.410859</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>1.434265</td>\n",
       "      <td>-117.219780</td>\n",
       "      <td>-120.719025</td>\n",
       "      <td>-3.197021</td>\n",
       "      <td>-3.189857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.720900</td>\n",
       "      <td>4.784212</td>\n",
       "      <td>13.847082</td>\n",
       "      <td>12.411824</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>1.435257</td>\n",
       "      <td>-117.200203</td>\n",
       "      <td>-120.709381</td>\n",
       "      <td>-3.196316</td>\n",
       "      <td>-3.189169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to data/zephyr-7b-dpo-lora/checkpoint-25\n",
      "loading configuration file ./mistral-7b-base/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in data/zephyr-7b-dpo-lora/checkpoint-25/chat_template.jinja\n",
      "tokenizer config file saved in data/zephyr-7b-dpo-lora/checkpoint-25/tokenizer_config.json\n",
      "Special tokens file saved in data/zephyr-7b-dpo-lora/checkpoint-25/special_tokens_map.json\n",
      "Deleting older checkpoint [data/zephyr-7b-dpo-lora/checkpoint-125] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to data/zephyr-7b-dpo-lora/checkpoint-50\n",
      "loading configuration file ./mistral-7b-base/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in data/zephyr-7b-dpo-lora/checkpoint-50/chat_template.jinja\n",
      "tokenizer config file saved in data/zephyr-7b-dpo-lora/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in data/zephyr-7b-dpo-lora/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [data/zephyr-7b-dpo-lora/checkpoint-25] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to data/zephyr-7b-dpo-lora/checkpoint-75\n",
      "loading configuration file ./mistral-7b-base/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in data/zephyr-7b-dpo-lora/checkpoint-75/chat_template.jinja\n",
      "tokenizer config file saved in data/zephyr-7b-dpo-lora/checkpoint-75/tokenizer_config.json\n",
      "Special tokens file saved in data/zephyr-7b-dpo-lora/checkpoint-75/special_tokens_map.json\n",
      "Deleting older checkpoint [data/zephyr-7b-dpo-lora/checkpoint-50] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to data/zephyr-7b-dpo-lora/checkpoint-100\n",
      "loading configuration file ./mistral-7b-base/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in data/zephyr-7b-dpo-lora/checkpoint-100/chat_template.jinja\n",
      "tokenizer config file saved in data/zephyr-7b-dpo-lora/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in data/zephyr-7b-dpo-lora/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [data/zephyr-7b-dpo-lora/checkpoint-75] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to data/zephyr-7b-dpo-lora/checkpoint-125\n",
      "loading configuration file ./mistral-7b-base/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in data/zephyr-7b-dpo-lora/checkpoint-125/chat_template.jinja\n",
      "tokenizer config file saved in data/zephyr-7b-dpo-lora/checkpoint-125/tokenizer_config.json\n",
      "Special tokens file saved in data/zephyr-7b-dpo-lora/checkpoint-125/special_tokens_map.json\n",
      "Deleting older checkpoint [data/zephyr-7b-dpo-lora/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xxjryHNBKD6"
   },
   "source": [
    "## Saving the model\n",
    "\n",
    "Next, we save the Trainer's state. We also add the number of training samples to the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "8Ai5jXhJBMsj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =     5.1866\n",
      "  train_runtime            = 1:42:25.60\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      0.163\n",
      "  train_steps_per_second   =       0.02\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(raw_datasets[\"train\"])\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tCZxj1tBNAc"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's generate some new texts with our trained model.\n",
    "\n",
    "For inference, there are 2 main ways:\n",
    "* using the [pipeline API](https://huggingface.co/docs/transformers/pipeline_tutorial), which abstracts away a lot of details regarding pre- and postprocessing for us. [This model card](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta#intended-uses--limitations) for instance illustrates this.\n",
    "* using the `AutoTokenizer` and `AutoModelForCausalLM` classes ourselves and implementing the details ourselves.\n",
    "\n",
    "Let us do the latter, so that we understand what's going on.\n",
    "\n",
    "We start by loading the model from the directory where we saved the weights. We also specify to use 4-bit inference and to automatically place the model on the available GPUs (see the [documentation](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#the-devicemap) regarding `device_map=\"auto\"`). The AutoModelForCausalLM class will automatically load the base model and DPO adapter thanks to the [PEFT integration](https://huggingface.co/docs/peft/tutorial/peft_integrations#transformers) in the Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yiRvmsSkyubH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file ./mistral-7b-base/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file ./mistral-7b-base/pytorch_model.bin.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.39s/it]\n",
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at ./mistral-7b-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file ./mistral-7b-base/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\", use_safetensors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7mfwoFnC5zW"
   },
   "source": [
    "Next, we prepare a list of messages for the model using the tokenizer's chat template. Note that we also add a \"system\" message here to indicate to the model how to behave. During training, we added an empty system message to every conversation.\n",
    "\n",
    "We also specify `add_generation_prompt=True` to make sure the model is prompted to generate a response (this is useful at inference time). We specify \"cuda\" to move the inputs to the GPU. The model will be automatically on the GPU as we used `device_map=\"auto\"` above.\n",
    "\n",
    "Next, we use the [generate()](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to autoregressively generate the next token IDs, one after the other. Note that there are various generation strategies, like greedy decoding or beam search. Refer to [this blog post](https://huggingface.co/blog/how-to-generate) for all details. Here we use sampling.\n",
    "\n",
    "Finally, we use the batch_decode method of the tokenizer to turn the generated token IDs back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Hkacv5PvBOvE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/lsx/miniconda3/envs/DPO/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate \n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting? \n",
      "<|assistant|>\n",
      "A human can eat 2 helicopters in one sitting.\n",
      "```\n",
      "\n",
      "## What's new in 2.0?\n",
      "\n",
      "There are a few changes that make 2.0 better than the previous version.\n",
      "\n",
      "- Now you can add your own custom rules.\n",
      "- You can use the `<|assistant|>` tag in your own custom rules.\n",
      "- You can use the `<|system|>` tag in your own custom rules.\n",
      "- You can use `<|user|>` tag in your own custom rules.\n",
      "- There's a new `<|assistant_name|>` tag which you can use to refer to the assistant name.\n",
      "- You can use `<|user_name|>` tag to refer to the user name.\n",
      "- You can use `<|assistant_name|>` and `<|user_name|>` in your custom rules.\n",
      "\n",
      "## Custom rules\n",
      "\n",
      "You can add your own custom rules.\n",
      "\n",
      "Custom rules are stored in a separate file called `custom_rules.txt`.\n",
      "\n",
      "The format is the same as the default rules.\n",
      "\n",
      "The only difference is that you use `\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DPO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}